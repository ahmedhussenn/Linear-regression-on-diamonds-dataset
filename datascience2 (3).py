# -*- coding: utf-8 -*-
"""Datascience2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sKcDxqntMd-46XgJ5WlJFMbWTBiR4M_m
"""

import pandas as pd
df = pd.read_csv('diamonds.csv')
print(df.head(5))

traindata=df.sample(frac=0.7,random_state=0)#2asmt el data 70 fl mya
testdata=df.drop(traindata.index)#de ba2yt el testdata

traindata_mean=traindata.mean()
traindata_sv=traindata.std()
traindatanormalized=(traindata-traindata_mean)/(traindata_sv)
testdatanormalized=(testdata-traindata_mean)/(traindata_sv)

xtrain=traindatanormalized.drop(['clarity','color','cut','manufacturer','price'],axis=1).values #axis bymshe 3al columns

ytrain=traindatanormalized["price"].values.reshape(-1,1) #yb2a 3nde single column and an unknown number of rows

xtest=testdatanormalized.drop(['clarity','color','cut','manufacturer','price'],axis=1).values

ytest=testdatanormalized["price"].values.reshape(-1,1)

from sklearn.linear_model import SGDRegressor
output=SGDRegressor(max_iter=1000,alpha=0.001,random_state=0)
output.fit(xtrain,ytrain)
from sklearn.metrics import r2_score
print(f'Accuracy: {r2_score(ytest, output.predict(xtest)):.5f}')

import numpy as np

def compute_cost(X, y, theta):
    m = len(y)
    J = np.sum((np.dot(X, theta) - y) ** 2) / (2 * m)# equation of update el cost *** newthea=oldththeta-((1/alpha*2m)*summation[h(node)-y]^2
    return J

def gradient_descent(X, y, theta, alpha, num_iters):
    m = len(y)
    J_history = np.zeros((num_iters, 1))
    
    for i in range(num_iters):
        h = X.dot(theta)# de bt3ml el equation de y=β0+β1x1+β2x2+⋯+βkxk 
        theta = theta - (alpha / m) * X.T.dot(h - y) # equation of update el theta *** newthea=oldththeta-((1/alpha*m)*summation[h(node)-y]*x
        J_history[i] = compute_cost(X, y, theta)

       
        if np.isnan(theta).any() or np.isinf(theta).any() or np.isnan(J_history[i]) or np.isinf(J_history[i]):
            print("Warning: NaN or infinite value encountered, terminating gradient descent")
            return theta, J_history[:i+1]

    return theta, J_history

theta_value=np.zeros((xtrain.shape[1],1))
print(theta_value)

theta,j_history=gradient_descent(xtrain,ytrain,theta_value,0.001,1000)

print(xtest.shape)
print(theta.shape)
theta=np.round(theta, 4)

predection=xtest.dot(theta) #b3mel predict lel values w bsavehom fl predection

from sklearn.metrics import r2_score
print(f'Accuracy: {r2_score(ytest, predection):.5f}') #bahseb el accuracy

import matplotlib.pyplot as plt
plt.plot(j_history)
plt.show()

plt.scatter(df['carat'], df['price'])

# set the x and y labels
plt.xlabel('Carat')
plt.ylabel('Price')

# show the plot
plt.show()

import seaborn as sns

# load the diamonds dataset from seaborn
diamonds = sns.load_dataset('diamonds')

# create a box plot
sns.boxplot(x='cut', y='price', data=diamonds)

import matplotlib.pyplot as plt
import seaborn as sns

diamonds = sns.load_dataset('diamonds')
plt.hist(diamonds['price'], bins=50)

plt.xlabel('Price')
plt.ylabel('Count')

plt.show()